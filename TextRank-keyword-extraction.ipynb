{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started:\n",
    "\n",
    "The input text (which is a random paragraph from the internet about graph-based ranking algorithms) is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"Graph-based ranking algorithms are essentially a way of deciding \\\n",
    "the importance of a vertex within a graph, based on global information recursively \\\n",
    "drawn from the entire graph. The basic idea implemented by a graph-based ranking model \\\n",
    "is that of “voting” or “recommendation”. When one vertex links to another one, it is \\\n",
    "basically casting a vote for that other vertex. The higher the number of votes that are \\\n",
    "cast for a vertex, the higher the importance of the vertex. Moreover, the importance of \\\n",
    "the vertex casting the vote determines how important the vote itself is, and this information \\\n",
    "is also taken into account by the ranking model. Hence, the score associated with a vertex is \\\n",
    "determined based on the votes that are cast for it, and the score of the vertices casting these votes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "The raw text input is converted to its lower case equivalent and is also ridden off any non-printable characters. The processed input text is then tokenized using NLTK library functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/blackred/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['graph-based', 'ranking', 'algorithms', 'are', 'essentially', 'a', 'way', 'of', 'deciding', 'the', 'importance', 'of', 'a', 'vertex', 'within', 'a', 'graph', ',', 'based', 'on', 'global', 'information', 'recursively', 'drawn', 'from', 'the', 'entire', 'graph', '.', 'the', 'basic', 'idea', 'implemented', 'by', 'a', 'graph-based', 'ranking', 'model', 'is', 'that', 'of', 'voting', 'or', 'recommendation', '.', 'when', 'one', 'vertex', 'links', 'to', 'another', 'one', ',', 'it', 'is', 'basically', 'casting', 'a', 'vote', 'for', 'that', 'other', 'vertex', '.', 'the', 'higher', 'the', 'number', 'of', 'votes', 'that', 'are', 'cast', 'for', 'a', 'vertex', ',', 'the', 'higher', 'the', 'importance', 'of', 'the', 'vertex', '.', 'moreover', ',', 'the', 'importance', 'of', 'the', 'vertex', 'casting', 'the', 'vote', 'determines', 'how', 'important', 'the', 'vote', 'itself', 'is', ',', 'and', 'this', 'information', 'is', 'also', 'taken', 'into', 'account', 'by', 'the', 'ranking', 'model', '.', 'hence', ',', 'the', 'score', 'associated', 'with', 'a', 'vertex', 'is', 'determined', 'based', 'on', 'the', 'votes', 'that', 'are', 'cast', 'for', 'it', ',', 'and', 'the', 'score', 'of', 'the', 'vertices', 'casting', 'these', 'votes', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text)\n",
    "    text = \"\".join(list(text))\n",
    "    return text\n",
    "\n",
    "Cleaned_text = clean(Text)\n",
    "text = word_tokenize(Cleaned_text)\n",
    "\n",
    "print (\"Tokenized Text: \\n\")\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging For Lemmatization\n",
    "\n",
    "As the next step, POS tagging is carried out on the input text (using the NTLK library again) so that the words can be lemmatized based on their POS tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/blackred/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n",
      "[('graph-based', 'JJ'), ('ranking', 'NN'), ('algorithms', 'NNS'), ('are', 'VBP'), ('essentially', 'RB'), ('a', 'DT'), ('way', 'NN'), ('of', 'IN'), ('deciding', 'VBG'), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('a', 'DT'), ('vertex', 'NN'), ('within', 'IN'), ('a', 'DT'), ('graph', 'NN'), (',', ','), ('based', 'VBN'), ('on', 'IN'), ('global', 'JJ'), ('information', 'NN'), ('recursively', 'RB'), ('drawn', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('graph', 'NN'), ('.', '.'), ('the', 'DT'), ('basic', 'JJ'), ('idea', 'NN'), ('implemented', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('graph-based', 'JJ'), ('ranking', 'NN'), ('model', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('of', 'IN'), ('voting', 'NN'), ('or', 'CC'), ('recommendation', 'NN'), ('.', '.'), ('when', 'WRB'), ('one', 'CD'), ('vertex', 'NN'), ('links', 'VBZ'), ('to', 'TO'), ('another', 'DT'), ('one', 'CD'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('basically', 'RB'), ('casting', 'VBG'), ('a', 'DT'), ('vote', 'NN'), ('for', 'IN'), ('that', 'DT'), ('other', 'JJ'), ('vertex', 'NN'), ('.', '.'), ('the', 'DT'), ('higher', 'JJR'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('votes', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('cast', 'VBN'), ('for', 'IN'), ('a', 'DT'), ('vertex', 'NN'), (',', ','), ('the', 'DT'), ('higher', 'JJR'), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertex', 'NN'), ('.', '.'), ('moreover', 'NN'), (',', ','), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertex', 'NN'), ('casting', 'VBG'), ('the', 'DT'), ('vote', 'NN'), ('determines', 'VBZ'), ('how', 'WRB'), ('important', 'JJ'), ('the', 'DT'), ('vote', 'NN'), ('itself', 'PRP'), ('is', 'VBZ'), (',', ','), ('and', 'CC'), ('this', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('taken', 'VBN'), ('into', 'IN'), ('account', 'NN'), ('by', 'IN'), ('the', 'DT'), ('ranking', 'JJ'), ('model', 'NN'), ('.', '.'), ('hence', 'NN'), (',', ','), ('the', 'DT'), ('score', 'NN'), ('associated', 'VBN'), ('with', 'IN'), ('a', 'DT'), ('vertex', 'NN'), ('is', 'VBZ'), ('determined', 'VBN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('votes', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('cast', 'VBN'), ('for', 'IN'), ('it', 'PRP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('score', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertices', 'NNS'), ('casting', 'VBG'), ('these', 'DT'), ('votes', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print (\"Tokenized Text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The tokenized text is now to be normalized by lemmatization.\n",
    "\n",
    "In lemmatization, different grammatical counterparts of a word will be replaced by single basic lemma (after removing any existing prefix or suffix of the word-mainly noun or adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/blackred/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['graph-based', 'ranking', 'algorithm', 'are', 'essentially', 'a', 'way', 'of', 'deciding', 'the', 'importance', 'of', 'a', 'vertex', 'within', 'a', 'graph', ',', 'based', 'on', 'global', 'information', 'recursively', 'drawn', 'from', 'the', 'entire', 'graph', '.', 'the', 'basic', 'idea', 'implemented', 'by', 'a', 'graph-based', 'ranking', 'model', 'is', 'that', 'of', 'voting', 'or', 'recommendation', '.', 'when', 'one', 'vertex', 'link', 'to', 'another', 'one', ',', 'it', 'is', 'basically', 'casting', 'a', 'vote', 'for', 'that', 'other', 'vertex', '.', 'the', 'high', 'the', 'number', 'of', 'vote', 'that', 'are', 'cast', 'for', 'a', 'vertex', ',', 'the', 'high', 'the', 'importance', 'of', 'the', 'vertex', '.', 'moreover', ',', 'the', 'importance', 'of', 'the', 'vertex', 'casting', 'the', 'vote', 'determines', 'how', 'important', 'the', 'vote', 'itself', 'is', ',', 'and', 'this', 'information', 'is', 'also', 'taken', 'into', 'account', 'by', 'the', 'ranking', 'model', '.', 'hence', ',', 'the', 'score', 'associated', 'with', 'a', 'vertex', 'is', 'determined', 'based', 'on', 'the', 'vote', 'that', 'are', 'cast', 'for', 'it', ',', 'and', 'the', 'score', 'of', 'the', 'vertex', 'casting', 'these', 'vote', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print (\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print (lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging for Filtering\n",
    "POS tagging of the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text with POS tags: \n",
      "\n",
      "[('graph-based', 'JJ'), ('ranking', 'NN'), ('algorithm', 'NNS'), ('are', 'VBP'), ('essentially', 'RB'), ('a', 'DT'), ('way', 'NN'), ('of', 'IN'), ('deciding', 'VBG'), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('a', 'DT'), ('vertex', 'NN'), ('within', 'IN'), ('a', 'DT'), ('graph', 'NN'), (',', ','), ('based', 'VBN'), ('on', 'IN'), ('global', 'JJ'), ('information', 'NN'), ('recursively', 'RB'), ('drawn', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('entire', 'JJ'), ('graph', 'NN'), ('.', '.'), ('the', 'DT'), ('basic', 'JJ'), ('idea', 'NN'), ('implemented', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('graph-based', 'JJ'), ('ranking', 'NN'), ('model', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('of', 'IN'), ('voting', 'NN'), ('or', 'CC'), ('recommendation', 'NN'), ('.', '.'), ('when', 'WRB'), ('one', 'CD'), ('vertex', 'NN'), ('link', 'NN'), ('to', 'TO'), ('another', 'DT'), ('one', 'CD'), (',', ','), ('it', 'PRP'), ('is', 'VBZ'), ('basically', 'RB'), ('casting', 'VBG'), ('a', 'DT'), ('vote', 'NN'), ('for', 'IN'), ('that', 'DT'), ('other', 'JJ'), ('vertex', 'NN'), ('.', '.'), ('the', 'DT'), ('high', 'JJ'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('vote', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('cast', 'VBN'), ('for', 'IN'), ('a', 'DT'), ('vertex', 'NN'), (',', ','), ('the', 'DT'), ('high', 'JJ'), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertex', 'NN'), ('.', '.'), ('moreover', 'NN'), (',', ','), ('the', 'DT'), ('importance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertex', 'NN'), ('casting', 'VBG'), ('the', 'DT'), ('vote', 'NN'), ('determines', 'VBZ'), ('how', 'WRB'), ('important', 'JJ'), ('the', 'DT'), ('vote', 'NN'), ('itself', 'PRP'), ('is', 'VBZ'), (',', ','), ('and', 'CC'), ('this', 'DT'), ('information', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('taken', 'VBN'), ('into', 'IN'), ('account', 'NN'), ('by', 'IN'), ('the', 'DT'), ('ranking', 'JJ'), ('model', 'NN'), ('.', '.'), ('hence', 'NN'), (',', ','), ('the', 'DT'), ('score', 'NN'), ('associated', 'VBN'), ('with', 'IN'), ('a', 'DT'), ('vertex', 'NN'), ('is', 'VBZ'), ('determined', 'VBN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('vote', 'NN'), ('that', 'WDT'), ('are', 'VBP'), ('cast', 'VBN'), ('for', 'IN'), ('it', 'PRP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('score', 'NN'), ('of', 'IN'), ('the', 'DT'), ('vertex', 'NN'), ('casting', 'VBG'), ('these', 'DT'), ('vote', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "\n",
    "print (\"Lemmatized text with POS tags: \\n\")\n",
    "print (POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Based Filtering\n",
    "The part where a list of the stop-words is derived for the given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword generation\n",
    "An external file (obtained from: https://www.ranks.nl/stopwords) made up of a long list of a lot of potential stopwords is added with our stopwords to create a final list (which is then converted to a set) to help removing stopwords that might have missed the POS based filtering and still be around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Removing stopwords from the lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graph-based', 'ranking', 'algorithm', 'deciding', 'vertex', 'graph', 'global', 'entire', 'graph', 'basic', 'idea', 'graph-based', 'ranking', 'model', 'voting', 'recommendation', 'vertex', 'link', 'casting', 'vote', 'vertex', 'high', 'number', 'vote', 'vertex', 'high', 'vertex', 'vertex', 'casting', 'vote', 'vote', 'account', 'ranking', 'model', 'score', 'vertex', 'vote', 'score', 'vertex', 'casting', 'vote']\n"
     ]
    }
   ],
   "source": [
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "print (processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Creation\n",
    "Vocabulary will only contain unique words from processed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vertex', 'idea', 'number', 'model', 'score', 'high', 'link', 'recommendation', 'global', 'casting', 'ranking', 'basic', 'account', 'voting', 'entire', 'graph-based', 'vote', 'deciding', 'algorithm', 'graph']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Graph\n",
    "Each word in the vocabulary will be a vertex for the graph. The words will be represented in the vertices by their index in the vocabulary list.\n",
    "\n",
    "The graph has wieghted undirected edges.\n",
    "\n",
    "So a weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating weighted summation of connections of a vertex\n",
    "inout[i] is to represent the sum of all the undirected connections/edges associated withe the vertex represented by i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Vertices\n",
    "The formula used for scoring a vertex represented by i is:\n",
    "\n",
    "score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ]\n",
    "    \n",
    "    where,\n",
    "    j belongs to the list of vertices that has a connection with i.\n",
    "    d is the damping factor.\n",
    "\n",
    "The score updated iteratively until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 26....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print(\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of vertex: 3.1234663\n",
      "Score of idea: 0.6992631\n",
      "Score of number: 0.5535985\n",
      "Score of model: 1.1075392\n",
      "Score of score: 0.9820695\n",
      "Score of high: 0.90221983\n",
      "Score of link: 0.5709196\n",
      "Score of recommendation: 0.6199509\n",
      "Score of global: 0.6905607\n",
      "Score of casting: 1.1087726\n",
      "Score of ranking: 1.5771186\n",
      "Score of basic: 0.7186171\n",
      "Score of account: 0.5832875\n",
      "Score of voting: 0.6277837\n",
      "Score of entire: 0.7181684\n",
      "Score of graph-based: 0.9347419\n",
      "Score of vote: 1.9325897\n",
      "Score of deciding: 0.63371617\n",
      "Score of algorithm: 0.6419196\n",
      "Score of graph: 1.2739041\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,vocab_len):\n",
    "    print(\"Score of \"+vocabulary[i]+\": \"+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Partiotioning\n",
    "Paritioning the lemmatized text into phrases using the stopwords in them as delimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['graph-based', 'ranking', 'algorithm'], ['deciding'], ['vertex'], ['graph'], ['global'], ['entire', 'graph'], ['basic', 'idea'], ['graph-based', 'ranking', 'model'], ['voting'], ['recommendation'], ['vertex', 'link'], ['casting'], ['vote'], ['vertex'], ['high'], ['number'], ['vote'], ['vertex'], ['high'], ['vertex'], ['vertex', 'casting'], ['vote'], ['vote'], ['account'], ['ranking', 'model'], ['score'], ['vertex'], ['vote'], ['score'], ['vertex', 'casting'], ['vote']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = \" \"\n",
    "for word in lemmatized_text:\n",
    "    \n",
    "    if word in stopwords_plus:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    elif word not in stopwords_plus:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list of unique phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['graph-based', 'ranking', 'algorithm'], ['deciding'], ['vertex'], ['graph'], ['global'], ['entire', 'graph'], ['basic', 'idea'], ['graph-based', 'ranking', 'model'], ['voting'], ['recommendation'], ['vertex', 'link'], ['casting'], ['vote'], ['high'], ['number'], ['vertex', 'casting'], ['account'], ['ranking', 'model'], ['score']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "\n",
    "print(\"Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinning the list of candidate-keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinned Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['graph-based', 'ranking', 'algorithm'], ['deciding'], ['global'], ['entire', 'graph'], ['basic', 'idea'], ['graph-based', 'ranking', 'model'], ['voting'], ['recommendation'], ['vertex', 'link'], ['vote'], ['high'], ['number'], ['vertex', 'casting'], ['account'], ['ranking', 'model'], ['score']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "            # and at the same time present as a word within a multi-worded phrase,\n",
    "            # then I will remove the single-word-phrase from the list.\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print(\"Thinned Unique Phrases (Candidate Keyphrases): \\n\")\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Keyphrases\n",
    "Scoring the phrases (candidate keyphrases) and building a list of key phrases/words by listing untokenized versions of the tokenized phrases/candidate-keyphrases.\n",
    "\n",
    "It is done by adding the score of their members (words/text-units that were ranked by the graph algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'graph-based ranking algorithm', Score: 3.153780162334442\n",
      "Keyword: 'deciding', Score: 0.6337161660194397\n",
      "Keyword: 'global', Score: 0.6905606985092163\n",
      "Keyword: 'entire graph', Score: 1.9920724630355835\n",
      "Keyword: 'basic idea', Score: 1.4178801774978638\n",
      "Keyword: 'graph-based ranking model', Score: 3.6193997263908386\n",
      "Keyword: 'voting', Score: 0.6277837157249451\n",
      "Keyword: 'recommendation', Score: 0.6199508905410767\n",
      "Keyword: 'vertex link', Score: 3.694385826587677\n",
      "Keyword: 'vote', Score: 1.9325896501541138\n",
      "Keyword: 'high', Score: 0.902219831943512\n",
      "Keyword: 'number', Score: 0.5535985231399536\n",
      "Keyword: 'vertex casting', Score: 4.2322388887405396\n",
      "Keyword: 'account', Score: 0.5832874774932861\n",
      "Keyword: 'ranking model', Score: 2.6846578121185303\n",
      "Keyword: 'score', Score: 0.9820694923400879\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print (\"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Keyphrases\n",
    "Ranking keyphrases based on their calculated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "vertex casting,  vertex link,  graph-based ranking model,  graph-based ranking algorithm,  ranking model,  entire graph,  vote,  basic idea,  score,  high,  "
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print(\"Keywords:\\n\")\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+\", \", end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output:\n",
    "\n",
    "(as obtained above)\n",
    "\n",
    "Keywords:\n",
    "\n",
    "    * vertex casting\n",
    "    * vertex link\n",
    "    * graph-based ranking model\n",
    "    * graph-based ranking algorithm\n",
    "    * ranking model\n",
    "    * entire graph\n",
    "    * vote\n",
    "    * basic idea\n",
    "    * score\n",
    "    * high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
